{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imdb+lstm.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPXJzCX6ibLRHD9MugKUmcI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexbrill/tf-train/blob/main/imdb%2Blstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzUs-GUcC5wO"
      },
      "source": [
        "# import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tCGl_Sy_52g"
      },
      "source": [
        "import numpy as np\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, LSTM, MaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A5nxbKJC8N9"
      },
      "source": [
        "# data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBz9Lw2w_-XG",
        "outputId": "3a5f375b-246a-42b0-f671-5c9f983e155c"
      },
      "source": [
        "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)\n",
        "data = np.concatenate((training_data, testing_data), axis=0)\n",
        "targets = np.concatenate((training_targets, testing_targets), axis=0)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/keras/datasets/imdb.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/keras/datasets/imdb.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wEQv7JmDBKv"
      },
      "source": [
        "# fitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sziU8qEGCiVz"
      },
      "source": [
        "max_review_length = 500\n",
        "\n",
        "X_train = sequence.pad_sequences(training_data, maxlen=max_review_length)\n",
        "y_train = training_targets\n",
        "X_test = sequence.pad_sequences(testing_data, maxlen=max_review_length)\n",
        "y_test = testing_targets"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f3ZlTsjDnXv"
      },
      "source": [
        "X_train[X_train >= 5000] = 0\n",
        "X_test[X_test >= 5000] = 0"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r4IbSPSFRvF"
      },
      "source": [
        "## LSTM "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mmk2CFnAvJm"
      },
      "source": [
        "top_words = 5000\n",
        "embedding_vector_length = 32\n",
        "max_review_length = 500\n",
        "\n",
        "def build(conv = False):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
        "  if conv:\n",
        "    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(LSTM(100))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvR1py7oA0mM",
        "outputId": "a37c943e-ec9f-4cfb-acda-57d039648ffc"
      },
      "source": [
        "model1 = build()\n",
        "model1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)\n",
        "scores = model1.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 213,301\n",
            "Trainable params: 213,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/3\n",
            "391/391 [==============================] - 38s 42ms/step - loss: 0.5608 - accuracy: 0.6854 - val_loss: 0.3208 - val_accuracy: 0.8664\n",
            "Epoch 2/3\n",
            "391/391 [==============================] - 16s 40ms/step - loss: 0.2869 - accuracy: 0.8860 - val_loss: 0.3411 - val_accuracy: 0.8668\n",
            "Epoch 3/3\n",
            "391/391 [==============================] - 15s 40ms/step - loss: 0.2395 - accuracy: 0.9091 - val_loss: 0.3723 - val_accuracy: 0.8377\n",
            "Accuracy: 83.77%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu7mWxxUFVie"
      },
      "source": [
        "## LSTM + CONV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1DEHdMUFbAg",
        "outputId": "df04fb57-55b9-4720-b100-4dca120437bf"
      },
      "source": [
        "model2 = build(conv=True)\n",
        "model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)\n",
        "scores = model2.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 500, 32)           3104      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 250, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 216,405\n",
            "Trainable params: 216,405\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/3\n",
            "391/391 [==============================] - 13s 28ms/step - loss: 0.5482 - accuracy: 0.6958 - val_loss: 0.2877 - val_accuracy: 0.8806\n",
            "Epoch 2/3\n",
            "391/391 [==============================] - 10s 27ms/step - loss: 0.2342 - accuracy: 0.9083 - val_loss: 0.3020 - val_accuracy: 0.8815\n",
            "Epoch 3/3\n",
            "391/391 [==============================] - 10s 26ms/step - loss: 0.2716 - accuracy: 0.8874 - val_loss: 0.3683 - val_accuracy: 0.8610\n",
            "Accuracy: 86.10%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJTtKuJeJKTJ"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCU_fRaxESrO"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-AdR98MEStT"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Wf6Oz_HESyz"
      },
      "source": [
        "# Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dvsd7waTDNpT"
      },
      "source": [
        "def ensemble(models, x):\n",
        "  return np.mean([model.predict(x) for model in models])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psL3BPSVFXnZ"
      },
      "source": [
        "def vectorize(sequences, dimension = 5000):\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    results[i, sequence] = 1\n",
        "\n",
        "  return results\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKRH2rPzDNrU",
        "outputId": "ace4502a-7b07-4124-ad11-ad897ae8edeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "index = imdb.get_word_index()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "1654784/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhJYmTwMGDmP"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KjUXJsQFe5j"
      },
      "source": [
        "def predict(source_text):\n",
        "  # prepocessing\n",
        "  text = source_text.lower().strip()\n",
        "\n",
        "  # tokenize\n",
        "  tokens = text.split()\n",
        "\n",
        "  # get indexes\n",
        "  indexes = [index.get(token, 0) for token in tokens]\n",
        "\n",
        "  # binarize\n",
        "  x = sequence.pad_sequences([indexes], maxlen=max_review_length)\n",
        "  x[x >= 5000] = 0\n",
        "\n",
        "  # get prediction\n",
        "  #pred = ensemble([model1, model2], binarized)\n",
        "  pred = np.mean([model1.predict(x), model2.predict(x)])\n",
        "\n",
        "  # result\n",
        "  if pred < 0.5:\n",
        "    return pred, 'neg'\n",
        "  else:\n",
        "    return pred, 'pos'"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "565OiyxOFm9l",
        "outputId": "b3dd4a1c-8891-4b4d-ac0e-ca9a6da2287f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "text = '''\n",
        "Bad film.\n",
        "I hate it. This film is awful. I hate it. It is OK.\n",
        "I hate it. This film is awful. I hate it. It is OK.\n",
        "I hate it. This film is awful. I hate it. It is OK.\n",
        "'''\n",
        "predict(text)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.2815455, 'neg')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ixq_6KxGFKV",
        "outputId": "e8c66e51-68c1-4233-fd1a-f31dd4607afc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "text = '''\n",
        "Good film.\n",
        "I love it. This film is amazing. I like it. It is not OK.\n",
        "I love it. This film is amazing. I like it. It is not OK.\n",
        "I love it. This film is amazing. I like it. It is not OK.\n",
        "'''\n",
        "predict(text)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7443879, 'pos')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    }
  ]
}